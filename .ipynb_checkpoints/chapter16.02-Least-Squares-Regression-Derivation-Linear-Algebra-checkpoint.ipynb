{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# 12.2 Dérivation de régression des moindres carrés (algèbre linéaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Tout d’abord, nous énumérons l’estimation des données à chaque point de données $x_i$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "&&\\hat{y}(x_1) = {\\alpha}_1 f_1(x_1) + {\\alpha}_2 f_2(x_1) + \\cdots + {\\alpha}_n f_n(x_1), \\\\\n",
    "&&\\hat{y}(x_2) = {\\alpha}_1 f_1(x_2) + {\\alpha}_2 f_2(x_2) + \\cdots + {\\alpha}_n f_n(x_2), \\\\\n",
    "&&\\qquad\\qquad \\qquad \\qquad \\quad \\cdots\\\\\n",
    "&&\\hat{y}(x_m) = {\\alpha}_1 f_1(x_m) + {\\alpha}_2 f_2(x_m) + \\cdots + {\\alpha}_n f_n(x_m).\\end{eqnarray*}\n",
    "\n",
    "Soit $X\\in {\\Bbb R}^n$ un vecteur colonne tel que le $i$-ème élément de $X$ contient la valeur du $i$-ème $x$-data point, $x_i, \\hat{Y}$ un vecteur colonne avec des éléments, $\\hat{Y}_i = \\hat{y}(x_i), {\\beta}$ un vecteur colonne tel que ${\\beta}_i = {\\alpha}_i, F_i(x)$ soit une fonction qui renvoie un vecteur colonne de $f_i(x)$ calculé sur chaque élément de $x$, et $A$ soit une matrice $m \\times n$ telle que la $i$-ème colonne de $A$ soit $F_i(x)$. Compte tenu de cette notation, le système d’équations précédent devient $\\hat{Y} = A{\\beta}$.\n",
    "\n",
    "Maintenant, si $Y$ est un vecteur colonne tel que $Y_i = y_i$, l'erreur quadratique totale est donnée par $E = \\|{\\hat{Y} - Y}\\|_{2}^2$. Vous pouvez le vérifier en substituant la définition de la norme $L_2$. Puisque nous voulons rendre $E$ aussi petit que possible et que les normes sont une mesure de distance, cette expression précédente équivaut à dire que nous voulons que $\\hat{Y}$ et $Y$ soient « aussi proches que possible ». Notez qu'en général, $Y$ ne sera pas dans la gamme de $A$ et donc $E > 0$.\n",
    "\n",
    "Considérons la représentation simplifiée suivante de la plage de $A$ ; voir la figure suivante. Notez qu'il s'agit de $\\it not$, un tracé des points de données $(x_i, y_i)$.\n",
    "\n",
    "<img src=\"./images/16.02.01-Illustration_of_Least_Square_Regression.png\" alt=\"Illustration of Least Square Regression\" title=\"Illustration of the L2 projection of Y on the range of A\" width=\"400\"/>\n",
    "\n",
    "D'après l'observation, le vecteur dans la plage de $A, \\hat{Y}$, qui est le plus proche de $Y$ est celui qui peut pointer perpendiculairement à $Y$. Par conséquent, nous voulons un vecteur $Y - \\hat{Y}$ perpendiculaire au vecteur $\\hat{Y}$.\n",
    "\n",
    "Rappelons en algèbre linéaire que deux vecteurs sont perpendiculaires, ou orthogonaux, si leur produit scalaire est 0. Notant que le produit scalaire entre deux vecteurs, $v$ et $w$, peut s'écrire ${\\text{dot}}(v,w) = v^T w$, nous pouvons affirmer que $\\hat{Y}$ et $Y - \\hat{Y}$ sont perpendiculaires si ${\\text{dot}}(\\hat{Y}, Y - \\hat{Y}) = 0$ ; donc $\\hat{Y}^T (Y - \\hat{Y}) = 0$, qui équivaut à $(A{\\beta})^T(Y - A{\\beta}) = 0$.\n",
    "\n",
    "Notant que pour deux matrices $A$ et $B, (AB)^T = B^T A^T$ et utilisant les propriétés distributives de multiplication vectorielle, cela équivaut à ${\\beta}^T A^T Y - {\\beta}^T A^T A {\\beta} = {\\beta}^T(A^T Y - A^T A {\\beta}) = 0$. La solution, ${\\beta} = \\textbf{0}$, est une solution triviale, nous utilisons donc $A^T Y - A^T A {\\beta} = 0$ pour trouver une solution plus intéressante. La résolution de cette équation pour ${\\beta}$ donne le $\\textbf{least squares regression formula}$ :\n",
    "\n",
    "$$\n",
    "{\\beta} = (A^T A)^{-1} A^T Y\n",
    "$$\n",
    "\n",
    "Notez que $(A^T A)^{-1}A^T$ est appelé le **pseudo-inverse** de $A$ et existe lorsque $m > n$ et $A$ ont des colonnes linéairement indépendantes. Prouver l'inversibilité de $(A^T A)$ sort du cadre de ce livre, mais il est toujours inversible sauf dans certains cas pathologiques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
