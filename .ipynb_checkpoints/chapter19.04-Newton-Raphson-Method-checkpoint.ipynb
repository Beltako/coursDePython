{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# 15.4 Méthode Newton-Raphson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Soit $f(x)$ une fonction fluide et continue et $x_r$ une racine inconnue de $f(x)$. Supposons maintenant que $x_0$ soit une supposition pour $x_r$. À moins que $x_0$ ne soit une supposition très chanceuse, $f(x_0)$ ne sera pas une racine. Compte tenu de ce scénario, nous voulons trouver un $x_1$ qui soit une amélioration par rapport à $x_0$ (c'est-à-dire plus proche de $x_r$ que de $x_0$). Si nous supposons que $x_0$ est \"assez proche\" de $x_r$, alors nous pouvons l'améliorer en prenant l'approximation linéaire de $f(x)$ autour de $x_0$, qui est une ligne, et en trouvant l'intersection de cette ligne avec l'axe des x. Écrit, l'approximation linéaire de $f(x)$ autour de $x_0$ est $f(x) \\approx f(x_0) + f^{\\prime}(x_0)(x-x_0)$. En utilisant cette approximation, on trouve $x_1$ tel que $f(x_1) = 0$. Brancher ces valeurs dans l'approximation linéaire donne l'équation\n",
    "\n",
    "$$\n",
    "0 = f(x_0) + f^{\\prime}(x_0)(x_1-x_0),\n",
    "$$\n",
    "qui, une fois résolu pour $x_1$, est\n",
    "$$\n",
    "x_1 = x_0 - \\frac{f(x_0)}{f^{\\prime}(x_0)}.\n",
    "$$\n",
    "\n",
    "Une illustration de la façon dont cette approximation linéaire améliore une estimation initiale est présentée dans la figure suivante.\n",
    " \n",
    "\n",
    "<img src=\"images/19.04.01-Newton-step.png\" alt=\"Newton Step\" title=\"Illustration of Newton step for a smooth function, g(x).\" width=\"200\"/>\n",
    "\n",
    "Écrit de manière générale, une **étape de Newton** calcule une estimation améliorée, $x_i$, en utilisant une estimation précédente $x_{i-1}$, et est donnée par l'équation\n",
    "\n",
    "$$\n",
    "x_i = x_{i-1} - \\frac{g(x_{i-1})}{g^{\\prime}(x_{i-1})}.\n",
    "$$\n",
    "\n",
    "La **Méthode Newton-Raphson** de recherche de racines itère les étapes de Newton à partir de $x_0$ jusqu'à ce que l'erreur soit inférieure à la tolérance.\n",
    "\n",
    "**ESSAYEZ-LE !** Encore une fois, le $\\sqrt{2}$ est la racine de la fonction $f(x) = x^2 - 2$. En utilisant $x_0 = 1.4$ comme point de départ, utilisez l’équation précédente pour estimer $\\sqrt{2}$. Comparez cette approximation avec la valeur calculée par la fonction sqrt de Python.\n",
    "\n",
    "$$\n",
    "x = 1.4 - \\frac{1.4^2 - 2}{2(1.4)} = 1.4142857142857144\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton_raphson = 1.4142857142857144\n",
      "sqrt(2) = 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = lambda x: x**2 - 2\n",
    "f_prime = lambda x: 2*x\n",
    "newton_raphson = 1.4 - (f(1.4))/(f_prime(1.4))\n",
    "\n",
    "print(\"newton_raphson =\", newton_raphson)\n",
    "print(\"sqrt(2) =\", np.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "**ESSAYEZ-LE !** Écrivez une fonction $my\\_newton(f, df, x0, tol)$, où la sortie est une estimation de la racine de *f*, *f* est un objet fonction $f(x)$, *df* est un objet fonction de $f^{\\prime}(x)$, *x0* est une estimation initiale et *tol* est la tolérance d'erreur. La mesure de l'erreur doit être $|f(x)|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def my_newton(f, df, x0, tol):\n",
    "    # output is an estimation of the root of f \n",
    "    # using the Newton Raphson method\n",
    "    # recursive implementation\n",
    "    if abs(f(x0)) < tol:\n",
    "        return x0\n",
    "    else:\n",
    "        return my_newton(f, df, x0 - f(x0)/df(x0), tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "**ESSAYEZ-LE !** Utilisez *my\\_newton=* pour calculer $\\sqrt{2}$ dans la tolérance de 1e-6 à partir de *x0 = 1,5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate = 1.4142135623746899\n",
      "sqrt(2) = 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "estimate = my_newton(f, f_prime, 1.5, 1e-6)\n",
    "print(\"estimate =\", estimate)\n",
    "print(\"sqrt(2) =\", np.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Si $x_0$ est proche de $x_r$, alors on peut prouver qu'en général, la méthode de Newton-Raphson converge vers $x_r$ beaucoup plus rapidement que la méthode de bissection. Cependant, comme $x_r$ est initialement inconnu, il n'y a aucun moyen de savoir si la supposition initiale est suffisamment proche de la racine pour obtenir ce comportement à moins que certaines informations spéciales sur la fonction ne soient connues *a priori* (par exemple, la fonction a une racine proche de $x = 0$). En plus de ce problème d’initialisation, la méthode Newton-Raphson présente d’autres limitations sérieuses. Par exemple, si la dérivée d'une estimation est proche de 0, alors le pas de Newton sera très grand et s'éloignera probablement de la racine. De plus, en fonction du comportement de la fonction dérivée entre $x_0$ et $x_r$, la méthode de Newton-Raphson peut converger vers une racine différente de celle de $x_r$ qui peut ne pas être utile pour notre application d'ingénierie.\n",
    "\n",
    "**ESSAYEZ-LE !** Calculez un seul pas de Newton pour obtenir une approximation améliorée de la racine de la fonction $f(x) = x^3 + 3x^2 - 2x - 5$ et une estimation initiale, $x_0 = 0.29$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 = -688.4516883116648\n"
     ]
    }
   ],
   "source": [
    "x0 = 0.29\n",
    "x1 = x0-(x0**3+3*x0**2-2*x0-5)/(3*x0**2+6*x0-2)\n",
    "print(\"x1 =\", x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Notez que $f^{\\prime}(x_0) = -0.0077$ (proche de 0) et l'erreur sur $x_1$ est d'environ 324880000 (très grande)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "**ESSAYEZ-LE !** Considérez le polynôme $f(x) = x^3 - 100x^2 - x + 100$. Ce polynôme a une racine en $x = 1$ et $x = 100$. Utilisez le Newton-Raphson pour trouver une racine de $f$ commençant à $x_0 = 0$.\n",
    "\n",
    "Chez $x_0 = 0, f(x_0) = 100$ et $f'(x) = -1$. Un pas de Newton donne $x_1 = 0 - \\frac{100}{-1} = 100$, qui est une racine de $f$. Cependant, notez que cette racine est beaucoup plus éloignée de la supposition initiale que l'autre racine de $x = 1$, et ce n'est peut-être pas la racine que vous vouliez à partir d'une supposition initiale de 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
