{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"images/book_cover.jpg\" width=\"120\">\n",
    "\n",
    "*Ce cahier contient un extrait de [Programmation Python et méthodes numériques - Un guide pour les ingénieurs et les scientifiques](https://pythonnumericalmethods.berkeley.edu/notebooks/Index.html), le contenu est également disponible sur [Berkeley Python Numerical Methods](https://pythonnumericalmethods.berkeley.edu/notebooks/Index.html).*\n",
    "\n",
    "*Les droits d'auteur du livre appartiennent à Elsevier. Nous avons également ce livre interactif en ligne pour une meilleure expérience d'apprentissage. Le code est publié sous la [licence MIT](https://opensource.org/licenses/MIT). Si vous trouvez ce contenu utile, pensez à soutenir le travail sur [Elsevier](https://www.elsevier.com/books/python-programming-and-numerical-methods/kong/978-0-12-819549-9) ou [Amazon](https://www.amazon.com/Python-Programming-Numerical-Methods-Scientists/dp/0128195495/ref=sr_1_1?dchild=1&keywords=Python+Programming+and+Numerical+Methods+-+A+Guide+for+Engineers+and+Scientists&qid=1604761352&sr=8-1) !*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [16.1 Least Squares Regression Problem Statement](chapter16.01-Least-Squares-Regression-Problem-Statement.ipynb)  | [Contents](Index.ipynb) | [16.3 Least Squares Regression Derivation (Multivariable Calculus)](chapter16.03-Least-Squares-Regression-Derivation-Multivariable-Calculus.ipynb)   >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# Dérivation de régression des moindres carrés (algèbre linéaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Tout d’abord, nous énumérons l’estimation des données à chaque point de données $x_i$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "&&\\hat{y}(x_1) = {\\alpha}_1 f_1(x_1) + {\\alpha}_2 f_2(x_1) + \\cdots + {\\alpha}_n f_n(x_1), \\\\\n",
    "&&\\hat{y}(x_2) = {\\alpha}_1 f_1(x_2) + {\\alpha}_2 f_2(x_2) + \\cdots + {\\alpha}_n f_n(x_2), \\\\\n",
    "&&\\qquad\\qquad \\qquad \\qquad \\quad \\cdots\\\\\n",
    "&&\\hat{y}(x_m) = {\\alpha}_1 f_1(x_m) + {\\alpha}_2 f_2(x_m) + \\cdots + {\\alpha}_n f_n(x_m).\\end{eqnarray*}\n",
    "\n",
    "Soit $X\\in {\\Bbb R}^n$ un vecteur colonne tel que le $i$-ème élément de $X$ contient la valeur du $i$-ème $x$-data point, $x_i, \\hat{Y}$ un vecteur colonne avec des éléments, $\\hat{Y}_i = \\hat{y}(x_i), {\\beta}$ un vecteur colonne tel que ${\\beta}_i = {\\alpha}_i, F_i(x)$ soit une fonction qui renvoie un vecteur colonne de $f_i(x)$ calculé sur chaque élément de $x$, et $A$ soit une matrice $m \\times n$ telle que la $i$-ème colonne de $A$ soit $F_i(x)$. Compte tenu de cette notation, le système d’équations précédent devient $\\hat{Y} = A{\\beta}$.\n",
    "\n",
    "Maintenant, si $Y$ est un vecteur colonne tel que $Y_i = y_i$, l'erreur quadratique totale est donnée par $E = \\|{\\hat{Y} - Y}\\|_{2}^2$. Vous pouvez le vérifier en substituant la définition de la norme $L_2$. Puisque nous voulons rendre $E$ aussi petit que possible et que les normes sont une mesure de distance, cette expression précédente équivaut à dire que nous voulons que $\\hat{Y}$ et $Y$ soient « aussi proches que possible ». Notez qu'en général, $Y$ ne sera pas dans la gamme de $A$ et donc $E > 0$.\n",
    "\n",
    "Considérons la représentation simplifiée suivante de la plage de $A$ ; voir la figure suivante. Notez qu'il s'agit de $\\it not$, un tracé des points de données $(x_i, y_i)$.\n",
    "\n",
    "<img src=\"./images/16.02.01-Illustration_of_Least_Square_Regression.png\" alt=\"Illustration of Least Square Regression\" title=\"Illustration of the L2 projection of Y on the range of A\" width=\"400\"/>\n",
    "\n",
    "D'après l'observation, le vecteur dans la plage de $A, \\hat{Y}$, qui est le plus proche de $Y$ est celui qui peut pointer perpendiculairement à $Y$. Par conséquent, nous voulons un vecteur $Y - \\hat{Y}$ perpendiculaire au vecteur $\\hat{Y}$.\n",
    "\n",
    "Rappelons en algèbre linéaire que deux vecteurs sont perpendiculaires, ou orthogonaux, si leur produit scalaire est 0. Notant que le produit scalaire entre deux vecteurs, $v$ et $w$, peut s'écrire ${\\text{dot}}(v,w) = v^T w$, nous pouvons affirmer que $\\hat{Y}$ et $Y - \\hat{Y}$ sont perpendiculaires si ${\\text{dot}}(\\hat{Y}, Y - \\hat{Y}) = 0$ ; donc $\\hat{Y}^T (Y - \\hat{Y}) = 0$, qui équivaut à $(A{\\beta})^T(Y - A{\\beta}) = 0$.\n",
    "\n",
    "Notant que pour deux matrices $A$ et $B, (AB)^T = B^T A^T$ et utilisant les propriétés distributives de multiplication vectorielle, cela équivaut à ${\\beta}^T A^T Y - {\\beta}^T A^T A {\\beta} = {\\beta}^T(A^T Y - A^T A {\\beta}) = 0$. La solution, ${\\beta} = \\textbf{0}$, est une solution triviale, nous utilisons donc $A^T Y - A^T A {\\beta} = 0$ pour trouver une solution plus intéressante. La résolution de cette équation pour ${\\beta}$ donne le $\\textbf{least squares regression formula}$ :\n",
    "\n",
    "$$\n",
    "{\\beta} = (A^T A)^{-1} A^T Y\n",
    "$$\n",
    "\n",
    "Notez que $(A^T A)^{-1}A^T$ est appelé le **pseudo-inverse** de $A$ et existe lorsque $m > n$ et $A$ ont des colonnes linéairement indépendantes. Prouver l'inversibilité de $(A^T A)$ sort du cadre de ce livre, mais il est toujours inversible sauf dans certains cas pathologiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [16.1 Least Squares Regression Problem Statement](chapter16.01-Least-Squares-Regression-Problem-Statement.ipynb)  | [Contents](Index.ipynb) | [16.3 Least Squares Regression Derivation (Multivariable Calculus)](chapter16.03-Least-Squares-Regression-Derivation-Multivariable-Calculus.ipynb)   >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
