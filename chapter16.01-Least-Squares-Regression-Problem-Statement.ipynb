{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"images/book_cover.jpg\" width=\"120\">\n",
    "\n",
    "*Ce cahier contient un extrait de [Programmation Python et méthodes numériques - Un guide pour les ingénieurs et les scientifiques](https://pythonnumericalmethods.berkeley.edu/notebooks/Index.html), le contenu est également disponible sur [Berkeley Python Numerical Methods](https://pythonnumericalmethods.berkeley.edu/notebooks/Index.html).*\n",
    "\n",
    "*Les droits d'auteur du livre appartiennent à Elsevier. Nous avons également ce livre interactif en ligne pour une meilleure expérience d'apprentissage. Le code est publié sous la [licence MIT](https://opensource.org/licenses/MIT). Si vous trouvez ce contenu utile, pensez à soutenir le travail sur [Elsevier](https://www.elsevier.com/books/python-programming-and-numerical-methods/kong/978-0-12-819549-9) ou [Amazon](https://www.amazon.com/Python-Programming-Numerical-Methods-Scientists/dp/0128195495/ref=sr_1_1?dchild=1&keywords=Python+Programming+and+Numerical+Methods+-+A+Guide+for+Engineers+and+Scientists&qid=1604761352&sr=8-1) !*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [CHAPTER 16.  Least Squares Regression](chapter16.00-Least-Squares-Regression.ipynb) | [Contents](Index.ipynb) | [16.2 Least Squares Regression Derivation (Linear Algebra)](chapter16.02-Least-Squares-Regression-Derivation-Linear-Algebra.ipynb)  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# Énoncé du problème de régression des moindres carrés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Étant donné un ensemble de points de données indépendants $x_i$ et de points de données dépendants $y_i, i = 1, \\ldots, m$, nous aimerions trouver une **fonction d'estimation**, $\\hat{y}(x)$, qui décrit au mieux les données. Notez que $\\hat{y}$ peut être fonction de plusieurs variables, mais pour les besoins de cette discussion, nous limitons le domaine de $\\hat{y}$ à une seule variable. Dans la régression des moindres carrés, la fonction d'estimation doit être une combinaison linéaire de **fonctions de base**, $f_i(x)$. Autrement dit, la fonction d’estimation doit être de la forme\n",
    "$$\n",
    "\\hat{y}(x) = \\sum_{i = 1}^n {\\alpha}_i f_i(x)\n",
    "$$\n",
    "Les scalaires ${\\alpha}_i$ sont appelés les **paramètres** de la fonction d'estimation, et chaque fonction de base doit être linéairement indépendante des autres. En d’autres termes, dans « l’espace fonctionnel » proprement dit, aucune fonction de base ne devrait être exprimable comme une combinaison linéaire des autres fonctions. Remarque : En général, il y a beaucoup plus de points de données, $m$, que de fonctions de base, $n$ (c'est-à-dire $m >> n$).\n",
    "\n",
    "**ESSAYEZ-LE !**\n",
    "Créez une fonction d'estimation pour la relation force-déplacement d'un ressort linéaire. Identifiez la ou les fonctions de base et les paramètres du modèle.\n",
    "\n",
    "La relation entre la force, $F$, et le déplacement, $x$, peut être décrite par la fonction $F(x) = kx$ où $k$ est la raideur du ressort. La seule fonction de base est la fonction $f_1(x) = x$ et le paramètre du modèle à rechercher est ${\\alpha}_1 = k$.\n",
    "\n",
    "Le but de la **régression des moindres carrés** est de trouver les paramètres de la fonction d'estimation qui minimisent l'**erreur quadratique totale**, $E$, définie par $E = \\sum_{i=1}^m (\\hat{y} - y_i)^2$. Les **erreurs individuelles** ou **résiduels** sont définis comme $e_i = (\\hat{y} - y_i)$. Si $e$ est le vecteur contenant toutes les erreurs individuelles, alors nous essayons également de minimiser $E = \\|{e}\\|_{2}^{2}$, qui est la norme $L_2$ définie dans le chapitre précédent.\n",
    "\n",
    "Dans les deux sections suivantes, nous dérivons la méthode des moindres carrés pour trouver les paramètres souhaités. La première dérivation vient de l’algèbre linéaire et la seconde dérivation vient du calcul multivarié. Bien qu’il s’agisse de dérivations différentes, elles conduisent à la même formule des moindres carrés. Vous êtes libre de vous concentrer sur la section avec laquelle vous êtes le plus à l’aise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [CHAPTER 16.  Least Squares Regression](chapter16.00-Least-Squares-Regression.ipynb) | [Contents](Index.ipynb) | [16.2 Least Squares Regression Derivation (Linear Algebra)](chapter16.02-Least-Squares-Regression-Derivation-Linear-Algebra.ipynb)  >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
